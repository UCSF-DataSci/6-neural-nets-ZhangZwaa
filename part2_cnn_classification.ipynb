{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "480a7bd7",
   "metadata": {},
   "source": [
    "# Part 2: CNN Classification\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this part, you'll implement a Convolutional Neural Network (CNN) for EMNIST character recognition. You can choose between TensorFlow/Keras or PyTorch for implementation. This will help you understand CNNs and their advantages for image classification tasks.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Implement a CNN using either TensorFlow/Keras or PyTorch\n",
    "- Apply convolutional layers, pooling, and batch normalization\n",
    "- Train and evaluate the model\n",
    "- Save model and metrics in the correct format\n",
    "\n",
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f6c515a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytest in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (8.3.5)\n",
      "Requirement already satisfied: nbformat in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 7)) (5.10.4)\n",
      "Requirement already satisfied: ipykernel in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 8)) (6.29.5)\n",
      "Requirement already satisfied: nbclient in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 9)) (0.10.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.5 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 12)) (2.2.3)\n",
      "Requirement already satisfied: numpy<3.0,>=1.20 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 13)) (2.1.3)\n",
      "Requirement already satisfied: scikit-learn<2.0,>=1.0 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 14)) (1.6.1)\n",
      "Requirement already satisfied: xgboost<3.0,>=1.5 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 15)) (2.1.4)\n",
      "Requirement already satisfied: imbalanced-learn<1.0,>=0.9 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 16)) (0.13.0)\n",
      "Requirement already satisfied: matplotlib<4.0,>=3.5 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 17)) (3.10.1)\n",
      "Requirement already satisfied: seaborn<1.0,>=0.11 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 18)) (0.13.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas<3.0,>=1.5->-r requirements.txt (line 12)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas<3.0,>=1.5->-r requirements.txt (line 12)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas<3.0,>=1.5->-r requirements.txt (line 12)) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn<2.0,>=1.0->-r requirements.txt (line 14)) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn<2.0,>=1.0->-r requirements.txt (line 14)) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn<2.0,>=1.0->-r requirements.txt (line 14)) (3.6.0)\n",
      "Requirement already satisfied: sklearn-compat<1,>=0.1 in ./.venv/lib/python3.12/site-packages (from imbalanced-learn<1.0,>=0.9->-r requirements.txt (line 16)) (0.1.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.12/site-packages (from matplotlib<4.0,>=3.5->-r requirements.txt (line 17)) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.12/site-packages (from matplotlib<4.0,>=3.5->-r requirements.txt (line 17)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.12/site-packages (from matplotlib<4.0,>=3.5->-r requirements.txt (line 17)) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib<4.0,>=3.5->-r requirements.txt (line 17)) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from matplotlib<4.0,>=3.5->-r requirements.txt (line 17)) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.12/site-packages (from matplotlib<4.0,>=3.5->-r requirements.txt (line 17)) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib<4.0,>=3.5->-r requirements.txt (line 17)) (3.2.3)\n",
      "Requirement already satisfied: iniconfig in ./.venv/lib/python3.12/site-packages (from pytest->-r requirements.txt (line 4)) (2.1.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in ./.venv/lib/python3.12/site-packages (from pytest->-r requirements.txt (line 4)) (1.5.0)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in ./.venv/lib/python3.12/site-packages (from nbformat->-r requirements.txt (line 7)) (2.21.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in ./.venv/lib/python3.12/site-packages (from nbformat->-r requirements.txt (line 7)) (4.23.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in ./.venv/lib/python3.12/site-packages (from nbformat->-r requirements.txt (line 7)) (5.7.2)\n",
      "Requirement already satisfied: traitlets>=5.1 in ./.venv/lib/python3.12/site-packages (from nbformat->-r requirements.txt (line 7)) (5.14.3)\n",
      "Requirement already satisfied: appnope in ./.venv/lib/python3.12/site-packages (from ipykernel->-r requirements.txt (line 8)) (0.1.4)\n",
      "Requirement already satisfied: comm>=0.1.1 in ./.venv/lib/python3.12/site-packages (from ipykernel->-r requirements.txt (line 8)) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in ./.venv/lib/python3.12/site-packages (from ipykernel->-r requirements.txt (line 8)) (1.8.14)\n",
      "Requirement already satisfied: ipython>=7.23.1 in ./.venv/lib/python3.12/site-packages (from ipykernel->-r requirements.txt (line 8)) (9.2.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in ./.venv/lib/python3.12/site-packages (from ipykernel->-r requirements.txt (line 8)) (8.6.3)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in ./.venv/lib/python3.12/site-packages (from ipykernel->-r requirements.txt (line 8)) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in ./.venv/lib/python3.12/site-packages (from ipykernel->-r requirements.txt (line 8)) (1.6.0)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.12/site-packages (from ipykernel->-r requirements.txt (line 8)) (7.0.0)\n",
      "Requirement already satisfied: pyzmq>=24 in ./.venv/lib/python3.12/site-packages (from ipykernel->-r requirements.txt (line 8)) (26.4.0)\n",
      "Requirement already satisfied: tornado>=6.1 in ./.venv/lib/python3.12/site-packages (from ipykernel->-r requirements.txt (line 8)) (6.4.2)\n",
      "Requirement already satisfied: decorator in ./.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 8)) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in ./.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 8)) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 8)) (0.19.2)\n",
      "Requirement already satisfied: pexpect>4.3 in ./.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 8)) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in ./.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 8)) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 8)) (2.19.1)\n",
      "Requirement already satisfied: stack_data in ./.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 8)) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in ./.venv/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel->-r requirements.txt (line 8)) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in ./.venv/lib/python3.12/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->-r requirements.txt (line 8)) (0.8.4)\n",
      "Requirement already satisfied: attrs>=22.2.0 in ./.venv/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat->-r requirements.txt (line 7)) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./.venv/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat->-r requirements.txt (line 7)) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./.venv/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat->-r requirements.txt (line 7)) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./.venv/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat->-r requirements.txt (line 7)) (0.24.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in ./.venv/lib/python3.12/site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat->-r requirements.txt (line 7)) (4.3.7)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./.venv/lib/python3.12/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->-r requirements.txt (line 8)) (0.7.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.5->-r requirements.txt (line 12)) (1.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in ./.venv/lib/python3.12/site-packages (from referencing>=0.28.4->jsonschema>=2.6->nbformat->-r requirements.txt (line 7)) (4.13.2)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./.venv/lib/python3.12/site-packages (from stack_data->ipython>=7.23.1->ipykernel->-r requirements.txt (line 8)) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./.venv/lib/python3.12/site-packages (from stack_data->ipython>=7.23.1->ipykernel->-r requirements.txt (line 8)) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in ./.venv/lib/python3.12/site-packages (from stack_data->ipython>=7.23.1->ipykernel->-r requirements.txt (line 8)) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install -r requirements.txt\n",
    "\n",
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib for better visualization\n",
    "# plt.style.use('seaborn')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('results/part_2', exist_ok=True)\n",
    "os.makedirs('logs', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39774d7e",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0c439591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (88800, 28, 28)\n",
      "Test data shape: (14800, 28, 28)\n",
      "Number of classes: 37\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load EMNIST 'letters' dataset\n",
    "(emnist_dataset_train, emnist_dataset_test), emnist_info = tfds.load(\n",
    "    'emnist/letters',\n",
    "    split=['train', 'test'],\n",
    "    as_supervised=True,\n",
    "    with_info=True\n",
    ")\n",
    "\n",
    "# Convert training set to NumPy arrays\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for img, label in tfds.as_numpy(emnist_dataset_train):\n",
    "    img = np.squeeze(img)  # Shape: (28, 28)\n",
    "    x_train.append(img)\n",
    "    y_train.append(label)\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Convert test set to NumPy arrays\n",
    "x_test = []\n",
    "y_test = []\n",
    "for img, label in tfds.as_numpy(emnist_dataset_test):\n",
    "    img = np.squeeze(img)\n",
    "    x_test.append(img)\n",
    "    y_test.append(label)\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Print dataset information\n",
    "print(f\"Training data shape: {x_train.shape}\")\n",
    "print(f\"Test data shape: {x_test.shape}\")\n",
    "print(f\"Number of classes: {emnist_info.features['label'].num_classes}\")\n",
    "\n",
    "# Plot sample images from training data\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i in range(5):\n",
    "    plt.subplot(1, 5, i + 1)\n",
    "    plt.imshow(x_train[i].T, cmap='gray')  # Transposed for correct orientation\n",
    "    plt.title(f'Label: {chr(y_train[i] + 64)}')  # EMNIST 'letters' labels: 1=A\n",
    "    plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d0c8d5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed training data shape: (71040, 28, 28, 1)\n",
      "Preprocessed validation data shape: (17760, 28, 28, 1)\n",
      "Preprocessed test data shape: (14800, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# Preprocess data\n",
    "# Normalize pixel values\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Reshape for CNN input (samples, height, width, channels)\n",
    "x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "x_test = x_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = tf.keras.utils.to_categorical(y_train - 1, num_classes=26)\n",
    "y_test = tf.keras.utils.to_categorical(y_test - 1, num_classes=26)\n",
    "\n",
    "# Split training data into train and validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_train, y_train, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Preprocessed training data shape: {x_train.shape}\")\n",
    "print(f\"Preprocessed validation data shape: {x_val.shape}\")\n",
    "print(f\"Preprocessed test data shape: {x_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb60786",
   "metadata": {},
   "source": [
    "## 2. Model Implementation\n",
    "\n",
    "### TensorFlow/Keras Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "2c369292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_24\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_24\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_48          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_49 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_49          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_49 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1600</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">204,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_49 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,354</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_48 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_48          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_48 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_49 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_49          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_49 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_24 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1600\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_48 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m204,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_24 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_49 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m)             │         \u001b[38;5;34m3,354\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">227,482</span> (888.60 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m227,482\u001b[0m (888.60 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">227,290</span> (887.85 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m227,290\u001b[0m (887.85 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> (768.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m192\u001b[0m (768.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create CNN using Keras\n",
    "def create_cnn_keras(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Create a CNN using TensorFlow/Keras.\n",
    "    \n",
    "    Requirements:\n",
    "    - Must use at least 2 convolutional layers\n",
    "    - Must include pooling and batch normalization\n",
    "    - Must use categorical crossentropy loss\n",
    "    \n",
    "    Goals:\n",
    "    - Achieve > 85% accuracy on test set\n",
    "    - Minimize overfitting using batch normalization and dropout\n",
    "    - Train efficiently with appropriate batch size and learning rate\n",
    "    \n",
    "    Args:\n",
    "        input_shape: Shape of input data (should be (28, 28, 1) for grayscale images)\n",
    "        num_classes: Number of output classes (26 for letters)\n",
    "    \n",
    "    Returns:\n",
    "        Compiled Keras model\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "            tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create and compile model\n",
    "model = create_cnn_keras(input_shape=(28, 28, 1), num_classes=26)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ba4d5d",
   "metadata": {},
   "source": [
    "### PyTorch Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "de7719ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=3136, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=26, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.shape[1:] == (28, 28, 1):\n",
    "            x = x.permute(0, 3, 1, 2)\n",
    "\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))  # [batch, 32, 14, 14]\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))  # [batch, 64, 7, 7]\n",
    "        x = torch.flatten(x, 1)                         # [batch, 64*7*7]\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model2 = CNN(num_classes=26)\n",
    "print(model2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99b0cf7",
   "metadata": {},
   "source": [
    "## 3. Training and Evaluation\n",
    "\n",
    "### TensorFlow/Keras Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34509b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m1915/2220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.6151 - loss: 1.3329"
     ]
    }
   ],
   "source": [
    "# Define callbacks\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=3\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        'models/cnn_keras.keras',\n",
    "        save_best_only=True\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=2,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot accuracy\n",
    "ax1.plot(history.history['accuracy'], label='Training')\n",
    "ax1.plot(history.history['val_accuracy'], label='Validation')\n",
    "ax1.set_title('Model Accuracy')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot loss\n",
    "ax2.plot(history.history['loss'], label='Training')\n",
    "ax2.plot(history.history['val_loss'], label='Validation')\n",
    "ax2.set_title('Model Loss')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d228b3c5",
   "metadata": {},
   "source": [
    "### PyTorch Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8326b232",
   "metadata": {},
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/homework/DS223/6-neural-nets-ZhangZwaa/.venv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:57\u001b[39m, in \u001b[36m_wrapfunc\u001b[39m\u001b[34m(obj, method, *args, **kwds)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m     59\u001b[39m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[32m     60\u001b[39m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     64\u001b[39m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[32m     65\u001b[39m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: argmax() got an unexpected keyword argument 'axis'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mAxisError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[114]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Convert data to PyTorch tensors\u001b[39;00m\n\u001b[32m      4\u001b[39m x_train = torch.FloatTensor(x_train).to(device)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m y_train = torch.LongTensor(\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m).to(device) \u001b[38;5;66;03m# Corrected line\u001b[39;00m\n\u001b[32m      6\u001b[39m x_val = torch.FloatTensor(x_val).to(device)\n\u001b[32m      7\u001b[39m y_val = torch.LongTensor(np.argmax(y_val, axis=\u001b[32m1\u001b[39m)).to(device)   \u001b[38;5;66;03m# Corrected line\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/homework/DS223/6-neural-nets-ZhangZwaa/.venv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:1359\u001b[39m, in \u001b[36margmax\u001b[39m\u001b[34m(a, axis, out, keepdims)\u001b[39m\n\u001b[32m   1270\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1271\u001b[39m \u001b[33;03mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[32m   1272\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1356\u001b[39m \u001b[33;03m(2, 1, 4)\u001b[39;00m\n\u001b[32m   1357\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1358\u001b[39m kwds = {\u001b[33m'\u001b[39m\u001b[33mkeepdims\u001b[39m\u001b[33m'\u001b[39m: keepdims} \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np._NoValue \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m-> \u001b[39m\u001b[32m1359\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43margmax\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/homework/DS223/6-neural-nets-ZhangZwaa/.venv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:66\u001b[39m, in \u001b[36m_wrapfunc\u001b[39m\u001b[34m(obj, method, *args, **kwds)\u001b[39m\n\u001b[32m     57\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(*args, **kwds)\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m     59\u001b[39m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[32m     60\u001b[39m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     64\u001b[39m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[32m     65\u001b[39m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/homework/DS223/6-neural-nets-ZhangZwaa/.venv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:46\u001b[39m, in \u001b[36m_wrapit\u001b[39m\u001b[34m(obj, method, *args, **kwds)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# As this already tried the method, subok is maybe quite reasonable here\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# but this follows what was done before. TODO: revisit this.\u001b[39;00m\n\u001b[32m     45\u001b[39m arr, = conv.as_arrays(subok=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m result = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m conv.wrap(result, to_scalar=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mAxisError\u001b[39m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "# Move model to device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Convert data to PyTorch tensors\n",
    "x_train = torch.FloatTensor(x_train).to(device)\n",
    "y_train = torch.LongTensor(np.argmax(y_train, axis=1)).to(device) # Corrected line\n",
    "x_val = torch.FloatTensor(x_val).to(device)\n",
    "y_val = torch.LongTensor(np.argmax(y_val, axis=1)).to(device)   # Corrected line\n",
    "model = model2.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "x_train = torch.FloatTensor(x_train).to(device)\n",
    "y_train = torch.LongTensor(np.argmax(y_train, axis=1)).to(device)\n",
    "x_val = torch.FloatTensor(x_val).to(device)\n",
    "y_val = torch.LongTensor(np.argmax(y_val, axis=1)).to(device)\n",
    "\n",
    "# Training loop\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(20):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    for i in range(0, len(x_train), 32):\n",
    "        batch_x = x_train[i:i+32]\n",
    "        batch_y = y_train[i:i+32]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        train_total += batch_y.size(0)\n",
    "        train_correct += predicted.eq(batch_y).sum().item()\n",
    "    \n",
    "    train_loss = train_loss / (len(x_train) / 32)\n",
    "    train_acc = train_correct / train_total\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(x_val), 32):\n",
    "            batch_x = x_val[i:i+32]\n",
    "            batch_y = y_val[i:i+32]\n",
    "            \n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_total += batch_y.size(0)\n",
    "            val_correct += predicted.eq(batch_y).sum().item()\n",
    "    \n",
    "    val_loss = val_loss / (len(x_val) / 32)\n",
    "    val_acc = val_correct / val_total\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        # Save best model\n",
    "        torch.save(model.state_dict(), 'models/cnn_pytorch.pt')\n",
    "        # Save architecture\n",
    "        with open('models/cnn_pytorch_arch.txt', 'w') as f:\n",
    "            f.write(str(model))\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            break\n",
    "\n",
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot accuracy\n",
    "ax1.plot(history['train_acc'], label='Training')\n",
    "ax1.plot(history['val_acc'], label='Validation')\n",
    "ax1.set_title('Model Accuracy')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot loss\n",
    "ax2.plot(history['train_loss'], label='Training')\n",
    "ax2.plot(history['val_loss'], label='Validation')\n",
    "ax2.set_title('Model Loss')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57c9833",
   "metadata": {},
   "source": [
    "## Progress Checkpoints\n",
    "\n",
    "1. **Data Loading**:\n",
    "   - [ ] Successfully load EMNIST dataset\n",
    "   - [ ] Verify data shapes and ranges\n",
    "   - [ ] Visualize sample images\n",
    "\n",
    "2. **Preprocessing**:\n",
    "   - [ ] Normalize pixel values\n",
    "   - [ ] Reshape data for CNN input\n",
    "   - [ ] Convert labels to one-hot encoding\n",
    "\n",
    "3. **Model Implementation**:\n",
    "   - [ ] Create CNN with required layers\n",
    "   - [ ] Verify architecture requirements\n",
    "   - [ ] Test model with sample input\n",
    "\n",
    "4. **Training**:\n",
    "   - [ ] Train model with callbacks\n",
    "   - [ ] Monitor training progress\n",
    "   - [ ] Save best model\n",
    "\n",
    "5. **Evaluation**:\n",
    "   - [ ] Calculate performance metrics\n",
    "   - [ ] Save metrics in correct format\n",
    "   - [ ] Visualize results\n",
    "\n",
    "## Common Issues and Solutions\n",
    "\n",
    "1. **Data Loading Issues**:\n",
    "   - Problem: EMNIST dataset not found\n",
    "   - Solution: Check internet connection and TensorFlow installation\n",
    "\n",
    "2. **Preprocessing Issues**:\n",
    "   - Problem: Shape mismatch in CNN layers\n",
    "   - Solution: Ensure data is properly shaped (samples, height, width, channels)\n",
    "   - Problem: Label encoding errors\n",
    "   - Solution: Verify label range and one-hot encoding\n",
    "\n",
    "3. **Model Issues**:\n",
    "   - Problem: Training instability\n",
    "   - Solution: Add batch normalization, reduce learning rate\n",
    "   - Problem: Overfitting\n",
    "   - Solution: Increase dropout, use data augmentation\n",
    "\n",
    "4. **Evaluation Issues**:\n",
    "   - Problem: Metrics format incorrect\n",
    "   - Solution: Follow the exact format specified\n",
    "   - Problem: Performance below threshold\n",
    "   - Solution: Adjust architecture, hyperparameters"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
